{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.layers import Dense, Input, Activation, multiply, Lambda\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.merge import add, concatenate\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from multiprocessing import Pool\n",
    "from scoop import futures\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all potential activation functions (Very problem specific)\n",
    "act_dict = {0: 'linear', 1: 'multiply', 2: 'inverse', 3: 'squared', 4: 'sqrt', 5: 'cubed'}\n",
    "\n",
    "np.random.seed(1000)\n",
    "weight_dict = {0: 0, 1: 1, 2: np.random.uniform(0,0.001,1)[0]}\n",
    "bias_dict = {0: 0, 1: np.random.uniform(0,0.001,1)[0]}\n",
    "#bias_dict = {0: 0, 1: 1.5}\n",
    "print(weight_dict)\n",
    "print(bias_dict)\n",
    "\n",
    "# Number of layers in the model (Includes input layer)\n",
    "nlayers = 4\n",
    "\n",
    "# Number of nodes per layer (Includes input layer)\n",
    "nNodes = [5, 5, 3, 1]\n",
    "\n",
    "# Number of variable weight/bias/activation terms to optimize\n",
    "nact_terms = sum(nNodes[1:])\n",
    "nweight_terms = sum([nNodes[i-1]*nNodes[i] for i in range(1, nlayers)])\n",
    "nbias_terms = nact_terms\n",
    "\n",
    "# Variable to load an old population as the starting population in the genetic algorithm (pickle file)\n",
    "Load_Old_Population = False\n",
    "\n",
    "# Parsinomy Value\n",
    "p = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to record what individuals have already been evaluated \n",
    "Individual_DB = []\n",
    "MSE_Test_DB = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Testing Data\n",
    "df = pd.read_csv(\"PropellantData_v3.csv\")\n",
    "#print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inputs\n",
    "inputs = np.array(df[['a','b', 'c', 'd', 'HeatofFormation']])\n",
    "inputs = inputs.astype(np.float)\n",
    "\n",
    "# Model output\n",
    "outputs = np.array(df['Isp']).reshape(-1, 1)\n",
    "\n",
    "inputs = np.around(inputs, decimals=6)\n",
    "outputs = np.around(outputs, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom square activation function\n",
    "def squared_act(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom cubed activation function\n",
    "def cubed_act(x):\n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom square root activation function (set up to handle negative inputs and train through them)\n",
    "def sqrt_act(x):\n",
    "    return tf.sign(x)* tf.sqrt(tf.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom inverse activation function\n",
    "def inverse_act(x):\n",
    "    return (1/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom multiply activation\n",
    "# Only includes non-zero weighted inputs to avoid only fully connected nodes returning non-zero outputs\n",
    "# x_inputs - description of node [A, w, w, w, ....., b] ex. [1, 2, 1, 2, 0, ..., 1]\n",
    "def custom_multiply(x, x_inputs):\n",
    "    # Lists to split the inputs tensors\n",
    "    activation_inputs = []\n",
    "    zero_inputs = []\n",
    "    \n",
    "    # splits the inputs tensors into zero tensor (zero incoming weight) and non zero tensor\n",
    "    for t in range(0, len(x)):\n",
    "        if x_inputs[t] != 0:\n",
    "            activation_inputs.append(x[t])\n",
    "        elif (t == len(x)-1) & (x_inputs[len(x)] == 1):\n",
    "            activation_inputs.append(x[t])\n",
    "        else:\n",
    "            zero_inputs.append(x[t])\n",
    "\n",
    "    # Checks if list is empty\n",
    "    if activation_inputs == []:\n",
    "        activation_tensor = 0\n",
    "    elif len(activation_inputs) == 1:\n",
    "        activation_tensor = activation_inputs[0]\n",
    "    else:\n",
    "        activation_tensor = multiply(activation_inputs)\n",
    "    \n",
    "    # Checks if list is empty\n",
    "    if zero_inputs == []:\n",
    "        zero_tensor = 0\n",
    "    elif len(zero_inputs) == 1:\n",
    "        zero_tensor = zero_inputs[0]\n",
    "    else:\n",
    "        zero_tensor = multiply(zero_inputs)\n",
    "        \n",
    "    # Checks if either list contains all the tensors\n",
    "    if not tf.is_tensor(zero_tensor):\n",
    "        return activation_tensor\n",
    "    elif not tf.is_tensor(activation_tensor):\n",
    "        return zero_tensor\n",
    "    else:\n",
    "        # Since all input tensors must be connected to the output, the zero tensors are added to the non zero tensors.\n",
    "        return add([activation_tensor, zero_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom keras layers\n",
    "# Allows for bias and weight to be set to trainable and non trainable independently\n",
    "# Normal keras dence layers \"trainable\" term controls both the bias and the weight\n",
    "class CustomDense(keras.layers.Layer):\n",
    "    def __init__(self, num_units, input_num, activation, name, trainable_weight, trainable_bias):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.activation = Activation(activation)\n",
    "        self.trainable_weight = trainable_weight\n",
    "        self.trainable_bias = trainable_bias\n",
    "        self.name = name\n",
    "        name_w = 'w'+self.name[1:]\n",
    "        name_b = 'b'+self.name[1:]\n",
    "        self.weight = self.add_weight(shape=(input_num, self.num_units), name=name_w, trainable=self.trainable_weight, initializer=\"zeros\")\n",
    "        self.bias = self.add_weight(shape=(self.num_units,), name=name_b, trainable=self.trainable_bias, initializer=\"zeros\")\n",
    "        \n",
    "    def call(self, input):\n",
    "        y = tf.matmul(input, self.weight) + self.bias\n",
    "        y = self.activation(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the nodes within the network\n",
    "def create_node(inputs, name, trainable, x_input, constraints):\n",
    "    base = name\n",
    "    act = act_dict[x_input[0]]\n",
    "  \n",
    "    # Collects inputs to the node and sets the connection to the appropriate trainable/bias setting\n",
    "    # Only the last connection to the node gets a bias (Every connection getting a bias is not useful)\n",
    "    an = []\n",
    "    n = []\n",
    "    for i in range(len(inputs)):\n",
    "        n = base + str(i + 1)\n",
    "        if i < len(inputs)-1:\n",
    "            an.append(CustomDense(1, 1, activation = 'linear', name=n, trainable_weight=trainable[i], trainable_bias=False) (inputs[i]))\n",
    "        else:\n",
    "            an.append(CustomDense(1, 1, activation = 'linear', name=n, trainable_weight=trainable[i], trainable_bias=trainable[len(trainable)-1]) (inputs[i]))\n",
    "\n",
    "    # Apply activation function to the list of inputs\n",
    "    if (act == \"multiply\"):\n",
    "        an = Activation(lambda x: custom_multiply(x, x_inputs=x_input[1:])) (an)\n",
    "    else:\n",
    "        an = add(an)\n",
    "        if (act == \"squared\"):\n",
    "            an = Activation(squared_act) (an)\n",
    "        elif (act == \"cubed\"):\n",
    "            an = Activation(cubed_act) (an)\n",
    "        elif (act == \"sqrt\"):\n",
    "            an = Activation(sqrt_act) (an)\n",
    "        elif (act == \"inverse\"):\n",
    "            an = Activation(inverse_act) (an)\n",
    "        else:\n",
    "            an = Activation(act) (an)\n",
    "            \n",
    "    # Checks for even root functions and connects output to constraint output node\n",
    "    if act == 'sqrt' or act == '4th_rt':\n",
    "        constraints.append(Activation('relu')(Lambda(lambda x: tf.negative(x))(an)))       \n",
    "    return an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x, constraint_strength=10^3):\n",
    "    #print(\"Individual: \", x)\n",
    "    constraints = []\n",
    "\n",
    "    # Collects a list of the trainable parameters in the model\n",
    "    trainable_list = []\n",
    "    for l in range(1, nlayers):\n",
    "        trainable_list.append([])\n",
    "        for n in range(nNodes[l]):\n",
    "            trainable_list[l-1].append([])\n",
    "            for i in range(1, nNodes[l-1]+1):\n",
    "                if (x[l-1][n][i] == 2):\n",
    "                    trainable_list[l-1][n].append(True)\n",
    "                else:\n",
    "                    trainable_list[l-1][n].append(False)\n",
    "            if (x[l-1][n][i+1] == 1):\n",
    "                trainable_list[l-1][n].append(True)\n",
    "            else:\n",
    "                trainable_list[l-1][n].append(False)\n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(nNodes[0]):\n",
    "        inputs.append(Input(shape=(1,)))\n",
    "    \n",
    "    # Builds list of customdense objects and connects each node to the previous layer\n",
    "    a = []\n",
    "    for l in range(1, nlayers):\n",
    "        a.append([])\n",
    "        for n in range(1, nNodes[l]+1):\n",
    "            a[l-1].append([])\n",
    "            if l == 1:\n",
    "                a[l-1][n-1] = create_node(inputs, 'a' + str(l) + str(n), trainable_list[l-1][n-1], x[l-1][n-1], constraints)\n",
    "            else:\n",
    "                a[l-1][n-1] = create_node(a[l-2], 'a' + str(l) + str(n), trainable_list[l-1][n-1], x[l-1][n-1], constraints)\n",
    "    \n",
    "    \n",
    "    # Make some adaptations depending on how many even root activation functions are used\n",
    "    if len(constraints) > 1:\n",
    "        constraint = add(constraints)\n",
    "    elif len(constraints) == 1:\n",
    "        constraint = constraints[0]\n",
    "    else:\n",
    "        constraint = Lambda(lambda x: x-x)(a[0][0])\n",
    "    \n",
    "    # Append constraint node\n",
    "    a[-1].append(constraint)\n",
    "    model = Model(inputs=inputs, outputs=a[-1])\n",
    "    \n",
    "    # Learning rate decay/schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-1,\n",
    "                                                                decay_steps=50000,\n",
    "                                                                decay_rate=0.1,\n",
    "                                                                staircase=False)\n",
    "    \n",
    "    # Optimizers\n",
    "    #optimizer = tf.keras.optimizers.SGD(lr=1e-2)\n",
    "    #optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    #optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr_schedule)\n",
    "    #optimizer = tf.keras.optimizers.Adagrad(learning_rate=lr_schedule)\n",
    "    #optimizer = tf.keras.optimizers.Adamax(learning_rate=lr_schedule)\n",
    "    #optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "    #optimizer = tf.keras.optimizers.Ftrl(lr=1e-3)\n",
    "    \n",
    "    \n",
    "    model.compile(loss=['mse','mse'], loss_weights=[1, constraint_strength], optimizer=optimizer)\n",
    "    model.layers.sort(key = lambda x: x.name, reverse=False)\n",
    "    \n",
    "    layer_list = []\n",
    "    for i in range(len(model.layers)):\n",
    "        name = model.layers[i].name\n",
    "        if ( (\"activation\" in name) or (\"input\" in name) or (\"add\" in name) or (\"multiply\" in name) or (\"lambda\" in name)):\n",
    "            continue\n",
    "        else:\n",
    "            layer_list.append(i)\n",
    "    \n",
    "    # Assign weights to the model\n",
    "    index = 0\n",
    "    for l in range(1, nlayers):\n",
    "        for n in range(1, nNodes[l]+1):\n",
    "            for i in range(1, nNodes[l-1]+1):\n",
    "                if ((i+1)%(nNodes[l-1]+1)==0):\n",
    "                    if (model.layers[layer_list[index]].get_weights()[0].shape==(1,1)):\n",
    "                        model.layers[layer_list[index]].set_weights( [ np.array( [[ weight_dict[x[l-1][n-1][i]] ]] ),  np.array( [ bias_dict[x[l-1][n-1][i+1]] ] ) ] )\n",
    "                    else:\n",
    "                        model.layers[layer_list[index]].set_weights( [ np.array( [ bias_dict[x[l-1][n-1][i+1]] ] ),  np.array( [[ weight_dict[x[l-1][n-1][i]] ]] ) ] )\n",
    "                else:\n",
    "                    if (model.layers[layer_list[index]].get_weights()[0].shape==(1,1)):\n",
    "                        model.layers[layer_list[index]].set_weights( [ np.array( [[ weight_dict[x[l-1][n-1][i]] ]] ),  np.array( [ 0 ] ) ] )\n",
    "                    else:\n",
    "                        model.layers[layer_list[index]].set_weights( [ np.array( [ 0 ] ),  np.array( [[ weight_dict[x[l-1][n-1][i]] ]] ) ] )\n",
    "                index += 1\n",
    "\n",
    "    return model, trainable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidLossNaN(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if np.isnan(logs.get('loss')):\n",
    "            self.model.stop_training=True\n",
    "\n",
    "losses = []\n",
    "class PrintEpNum(keras.callbacks.Callback): # This is a function for the Epoch Counter\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"Current Epoch: \" + str(epoch+1) + ' Loss: ' + str(logs.get('loss')) + '                     \\n')\n",
    "        losses.append(logs.get('loss'))\n",
    "\n",
    "def train(model, train_inputs, train_outputs, verbose=False):\n",
    "    mae_es= keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                          min_delta=1e-8, verbose=0, mode='auto', restore_best_weights=True)\n",
    "    \n",
    "    terminate = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "    EPOCHS = 50000 # Number of EPOCHS\n",
    "    history = model.fit([train_inputs[:, i] for i in range(0, nNodes[0])], [train_outputs, np.zeros(train_outputs.shape)], epochs=EPOCHS,\n",
    "                        shuffle=False, batch_size=32, verbose = 0, callbacks=[ValidLossNaN(), terminate, mae_es],\n",
    "                        validation_split=0.3)\n",
    "    # Test changing batch size\n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Sq Error')\n",
    "        plt.plot(history.epoch, np.array(history.history['loss']),label='Training loss')\n",
    "        plt.plot(history.epoch, np.array(history.history['val_loss']),label='Val loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFolds function to evaluate the model\n",
    "def cv_error(individual, inputs, outputs):\n",
    "    # Get splits for test train\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    kf.get_n_splits(inputs)\n",
    "    \n",
    "    cv_mse_list = []\n",
    "    \n",
    "\n",
    "    for train_index, test_index in kf.split(inputs):\n",
    "        # Create a model with initial weights\n",
    "        # Weights are not saved across each split\n",
    "        new_model, trainable = create_model(individual)\n",
    "        \n",
    "        # Train model\n",
    "        if (any(trainable) == True):\n",
    "            try:\n",
    "                trained = True\n",
    "                train(new_model, inputs[train_index, :], outputs[train_index], verbose=0)\n",
    "            except:\n",
    "                trained = False\n",
    "                new_model, trainable = create_model(individual)\n",
    "    \n",
    "        # Get weights/biases\n",
    "        weights = new_model.get_weights()\n",
    "        weight_list = []\n",
    "        bias_list = []\n",
    "        for weight in weights:\n",
    "            if (weight.shape == (1,1)):\n",
    "                weight_list.append(weight[0])\n",
    "            else:\n",
    "                bias_list.append(weight[0])\n",
    "        weight_list = np.array(weight_list)\n",
    "        bias_list = np.array(bias_list)\n",
    "    \n",
    "        #handle nan weights and biases\n",
    "        if (np.isnan(weight_list).any()):\n",
    "            cv_mse = 1e50\n",
    "        elif (np.isnan(bias_list).any()):\n",
    "            cv_mse = 1e50\n",
    "        elif not trained:\n",
    "            cv_mse = 1e50\n",
    "        else:\n",
    "            cv_mse = new_model.evaluate([inputs[test_index, i] for i in range(0, nNodes[0])], [outputs[test_index], np.zeros(outputs[test_index].shape)], verbose=0)\n",
    "            if (np.isnan(cv_mse).any()):\n",
    "                cv_mse = 1e50\n",
    "            else:\n",
    "                cv_mse = cv_mse[1]\n",
    "                cv_mse = np.around(cv_mse, decimals=6)\n",
    "        cv_mse_list.append(cv_mse)\n",
    "    return np.mean(cv_mse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(w):\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_reader(temp):\n",
    "    act_terms = temp[:nact_terms]\n",
    "    weight_terms = temp[nact_terms:nact_terms + nweight_terms]\n",
    "    bias_terms = temp[nact_terms + nweight_terms:]\n",
    "    print(weight_terms)\n",
    "    x = []\n",
    "    for l in range(1, nlayers):\n",
    "        x.append([])\n",
    "        for n in range(1, nNodes[l]+1):\n",
    "            x[l-1].append([])\n",
    "            x[l-1][n-1].append(act_terms.pop(0))\n",
    "            for i in range(1, nNodes[l-1]+1):\n",
    "                x[l-1][n-1].append(weight_terms.pop(0))\n",
    "            x[l-1][n-1].append(bias_terms.pop(0))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(individual):\n",
    "    gen = individual.pop()\n",
    "    \n",
    "    if individual not in Individual_DB:\n",
    "        # Evaluate model\n",
    "        mse_test = cv_error(individual, inputs, outputs)\n",
    "        \n",
    "        # Determine model complexity\n",
    "        actfunc_term = 0\n",
    "        wtbs_term = 0\n",
    "        for l in range(1, nlayers):\n",
    "            for n in range(nNodes[l]):\n",
    "                actfunc_term += individual[l-1][n][0]\n",
    "                wtbs_term += np.sum(individual[l-1][n][1:nNodes[l-1]+1])\n",
    "                wtbs_term += individual[l-1][n][nNodes[l-1]+1]*2\n",
    "                \n",
    "        # Main Objective function for GA    \n",
    "        obj = mse_test + p*(np.sum(actfunc_term) + wtbs_term)\n",
    "        #obj = mse_test_term\n",
    "        print (\"Individual: \", individual, flush=True)\n",
    "        print (\"Objective function: \", mse_test, np.sum(actfunc_term), wtbs_term, obj, gen, flush=True)\n",
    "        \n",
    "        Individual_DB.append(individual)\n",
    "        MSE_Test_DB.append(mse_test)\n",
    "        \n",
    "    else:\n",
    "        # Evaluate model\n",
    "        mse_test = MSE_Test_DB[Individual_DB.index(individual)]\n",
    "        \n",
    "        # Determine model complexity\n",
    "        actfunc_term = 0\n",
    "        wtbs_term = 0\n",
    "        for l in range(1, nlayers):\n",
    "            for n in range(nNodes[l]):\n",
    "                actfunc_term += individual[l-1][n][0]\n",
    "                wtbs_term += np.sum(individual[l-1][n][1:nNodes[l-1]+1])\n",
    "                wtbs_term += individual[l-1][n][nNodes[l-1]+1]*2\n",
    "                \n",
    "        # Main Objective function for GA\n",
    "        obj = mse_test + p*(np.sum(actfunc_term) + wtbs_term)\n",
    "    \n",
    "    K.clear_session()\n",
    "    #tf.reset_default_graph()\n",
    "    return (obj,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create random individuals\n",
    "def custom_initRepeat(container, func, max1, max2, max3):\n",
    "    x = []\n",
    "    for l in range(1, nlayers):\n",
    "        x.append([])\n",
    "        for n in range(1, nNodes[l]+1):\n",
    "            x[l-1].append([])\n",
    "            x[l-1][n-1].append(func(0, max1))\n",
    "            for i in range(1, nNodes[l-1]+1):\n",
    "                x[l-1][n-1].append(func(0, max2))\n",
    "            x[l-1][n-1].append(func(0, max3))\n",
    "    return container(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GA mutation. Only mutate each activation, weight, bias to a valid setting\n",
    "def custom_mutation(individual, indpb, max1, max2, max3):\n",
    "    for l in range(1, nlayers):\n",
    "        for n in range(1, nNodes[l]+1):\n",
    "            if random.random() < indpb:\n",
    "                individual[l-1][n-1][0] = random.randint(0, max1)\n",
    "            for i in range(1, nNodes[l-1]+1):\n",
    "                if random.random() < indpb:\n",
    "                    individual[l-1][n-1][i] = random.randint(0, max2)\n",
    "            if random.random() < indpb:\n",
    "                individual[l-1][n-1][i+1] = random.randint(0, max3)\n",
    "    return individual,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GA crossover. Switches nodes from the same location on each network\n",
    "def custom_crossover(individual1, individual2):\n",
    "    LAYER = random.randint(1, nlayers-1)\n",
    "    NODE = random.randint(0, nNodes[LAYER]-1)\n",
    "    temp = individual1[LAYER-1][NODE]\n",
    "    individual1[LAYER-1][NODE] = individual2[LAYER-1][NODE]\n",
    "    individual2[LAYER-1][NODE] = temp\n",
    "    return individual1, individual2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eaSimple(population, toolbox, cxpb, mutpb, ngen, stats=None,\n",
    "             halloffame=None, verbose=__debug__):\n",
    "    \"\"\"This algorithm reproduce the simplest evolutionary algorithm as\n",
    "    presented in chapter 7 of [Back2000]_.\n",
    "\n",
    "    :param population: A list of individuals.\n",
    "    :param toolbox: A :class:`~deap.base.Toolbox` that contains the evolution\n",
    "                    operators.\n",
    "    :param cxpb: The probability of mating two individuals.\n",
    "    :param mutpb: The probability of mutating an individual.\n",
    "    :param ngen: The number of generation.\n",
    "    :param stats: A :class:`~deap.tools.Statistics` object that is updated\n",
    "                  inplace, optional.\n",
    "    :param halloffame: A :class:`~deap.tools.HallOfFame` object that will\n",
    "                       contain the best individuals, optional.\n",
    "    :param verbose: Whether or not to log the statistics.\n",
    "    :returns: The final population\n",
    "    :returns: A class:`~deap.tools.Logbook` with the statistics of the\n",
    "              evolution\n",
    "\n",
    "    The algorithm takes in a population and evolves it in place using the\n",
    "    :meth:`varAnd` method. It returns the optimized population and a\n",
    "    :class:`~deap.tools.Logbook` with the statistics of the evolution. The\n",
    "    logbook will contain the generation number, the number of evaluations for\n",
    "    each generation and the statistics if a :class:`~deap.tools.Statistics` is\n",
    "    given as argument. The *cxpb* and *mutpb* arguments are passed to the\n",
    "    :func:`varAnd` function. The pseudocode goes as follow ::\n",
    "\n",
    "        evaluate(population)\n",
    "        for g in range(ngen):\n",
    "            population = select(population, len(population))\n",
    "            offspring = varAnd(population, toolbox, cxpb, mutpb)\n",
    "            evaluate(offspring)\n",
    "            population = offspring\n",
    "\n",
    "    As stated in the pseudocode above, the algorithm goes as follow. First, it\n",
    "    evaluates the individuals with an invalid fitness. Second, it enters the\n",
    "    generational loop where the selection procedure is applied to entirely\n",
    "    replace the parental population. The 1:1 replacement ratio of this\n",
    "    algorithm **requires** the selection procedure to be stochastic and to\n",
    "    select multiple times the same individual, for example,\n",
    "    :func:`~deap.tools.selTournament` and :func:`~deap.tools.selRoulette`.\n",
    "    Third, it applies the :func:`varAnd` function to produce the next\n",
    "    generation population. Fourth, it evaluates the new individuals and\n",
    "    compute the statistics on this population. Finally, when *ngen*\n",
    "    generations are done, the algorithm returns a tuple with the final\n",
    "    population and a :class:`~deap.tools.Logbook` of the evolution.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        Using a non-stochastic selection method will result in no selection as\n",
    "        the operator selects *n* individuals from a pool of *n*.\n",
    "\n",
    "    This function expects the :meth:`toolbox.mate`, :meth:`toolbox.mutate`,\n",
    "    :meth:`toolbox.select` and :meth:`toolbox.evaluate` aliases to be\n",
    "    registered in the toolbox.\n",
    "\n",
    "    .. [Back2000] Back, Fogel and Michalewicz, \"Evolutionary Computation 1 :\n",
    "       Basic Algorithms and Operators\", 2000.\n",
    "    \"\"\"\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (stats.fields if stats else [])\n",
    "\n",
    "    # Evaluate the individuals with an invalid fitness\n",
    "    invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "    for ind in invalid_ind:\n",
    "        ind.append(0)\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    if halloffame is not None:\n",
    "        halloffame.update(population)\n",
    "\n",
    "    record = stats.compile(population) if stats else {}\n",
    "    logbook.record(gen=0, nevals=len(invalid_ind), **record)\n",
    "    if verbose:\n",
    "        print(logbook.stream)\n",
    "\n",
    "    # Begin the generational process\n",
    "    for gen in range(1, ngen + 1):\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population))\n",
    "\n",
    "        # Vary the pool of individuals\n",
    "        offspring = custom_varAnd(offspring, toolbox, cxpb, mutpb, gen, ngen)\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        for ind in invalid_ind:\n",
    "            ind.append(gen)\n",
    "        fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "            if len(ind) > (nlayers-1):\n",
    "                ind = ind.pop()\n",
    "\n",
    "        # Update the hall of fame with the generated individuals\n",
    "        if halloffame is not None:\n",
    "            halloffame.update(offspring)\n",
    "\n",
    "        # Replace the current population by the offspring\n",
    "        population[:] = offspring\n",
    "\n",
    "        # Append the current generation statistics to the logbook\n",
    "        record = stats.compile(population) if stats else {}\n",
    "        logbook.record(gen=gen, nevals=len(invalid_ind), **record)\n",
    "        if verbose:\n",
    "            print(logbook.stream)\n",
    "\n",
    "    return population, logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_varAnd(population, toolbox, cxpb, mutpb, gen, ngen):\n",
    "    \"\"\"Part of an evolutionary algorithm applying only the variation part\n",
    "    (crossover **and** mutation). The modified individuals have their\n",
    "    fitness invalidated. The individuals are cloned so returned population is\n",
    "    independent of the input population.\n",
    "\n",
    "    :param population: A list of individuals to vary.\n",
    "    :param toolbox: A :class:`~deap.base.Toolbox` that contains the evolution\n",
    "                    operators.\n",
    "    :param cxpb: The probability of mating two individuals.\n",
    "    :param mutpb: The probability of mutating an individual.\n",
    "    :returns: A list of varied individuals that are independent of their\n",
    "              parents.\n",
    "\n",
    "    The variation goes as follow. First, the parental population\n",
    "    :math:`P_\\mathrm{p}` is duplicated using the :meth:`toolbox.clone` method\n",
    "    and the result is put into the offspring population :math:`P_\\mathrm{o}`.  A\n",
    "    first loop over :math:`P_\\mathrm{o}` is executed to mate pairs of\n",
    "    consecutive individuals. According to the crossover probability *cxpb*, the\n",
    "    individuals :math:`\\mathbf{x}_i` and :math:`\\mathbf{x}_{i+1}` are mated\n",
    "    using the :meth:`toolbox.mate` method. The resulting children\n",
    "    :math:`\\mathbf{y}_i` and :math:`\\mathbf{y}_{i+1}` replace their respective\n",
    "    parents in :math:`P_\\mathrm{o}`. A second loop over the resulting\n",
    "    :math:`P_\\mathrm{o}` is executed to mutate every individual with a\n",
    "    probability *mutpb*. When an individual is mutated it replaces its not\n",
    "    mutated version in :math:`P_\\mathrm{o}`. The resulting :math:`P_\\mathrm{o}`\n",
    "    is returned.\n",
    "\n",
    "    This variation is named *And* because of its propensity to apply both\n",
    "    crossover and mutation on the individuals. Note that both operators are\n",
    "    not applied systematically, the resulting individuals can be generated from\n",
    "    crossover only, mutation only, crossover and mutation, and reproduction\n",
    "    according to the given probabilities. Both probabilities should be in\n",
    "    :math:`[0, 1]`.\n",
    "    \"\"\"\n",
    "    offspring = [toolbox.clone(ind) for ind in population]\n",
    "\n",
    "    # Apply crossover and mutation on the offspring\n",
    "    for i in range(1, len(offspring), 2):\n",
    "        if random.random() < cxpb:\n",
    "            offspring[i - 1], offspring[i] = toolbox.mate(offspring[i - 1],\n",
    "                                                          offspring[i])\n",
    "            del offspring[i - 1].fitness.values, offspring[i].fitness.values\n",
    "\n",
    "    for i in range(len(offspring)):\n",
    "        if random.random() < mutpb[(gen-1)//(ngen//len(mutpb))]:\n",
    "            offspring[i], = toolbox.mutate(offspring[i], mutpb[(gen-1)//(ngen//len(mutpb))])\n",
    "            del offspring[i].fitness.values\n",
    "            \n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### DEAP #####################\n",
    "#create fitness class and individual class\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "#pool = Pool(1)\n",
    "toolbox.register(\"map\", futures.map)\n",
    "#toolbox.register(\"attr_int\", random.randint, 0, 3)\n",
    "\n",
    "#gen = initRepeat(list, random.randint, 3, 7, 4)\n",
    "toolbox.register(\"create_individual\", custom_initRepeat, creator.Individual, random.randint, \n",
    "                 max1=5, max2=2, max3=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.create_individual)\n",
    "\n",
    "# Crossover probability\n",
    "cxpb = 0.5\n",
    "# Variable mutation (decreases across generations)\n",
    "# Length of mutpb list must divide evenly into ngens\n",
    "mutpb = [0.9, 0.8, 0.5, 0.2, 0.1]\n",
    "if Load_Old_Population:\n",
    "    mutpb = [0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "# Number of generations\n",
    "ngens = 5\n",
    "\n",
    "toolbox.register(\"mate\", custom_crossover)\n",
    "#toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=3, indpb=mutpb)\n",
    "toolbox.register(\"mutate\", custom_mutation, max1=3, max2=2, max3=1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=80)\n",
    "toolbox.register(\"evaluate\", objective_function)\n",
    "\n",
    "def main():\n",
    "    random.seed(10000)\n",
    "    \n",
    "    if not Load_Old_Population:\n",
    "        population = toolbox.population(n=5)\n",
    "    else:\n",
    "        with open(\"P\" + str(p) + \"_1\", 'rb') as f:\n",
    "            population = pickle.load(f)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "    pop, logbook = custom_eaSimple(population, toolbox, cxpb, mutpb, ngens, stats=stats, halloffame=hof, verbose=True)\n",
    "\n",
    "    return pop, logbook, hof\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pop, logbook, hof = main()\n",
    "    \n",
    "    # Save final population\n",
    "    filename = \"P\" + str(p)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(pop, f)\n",
    "        f.close()\n",
    "        \n",
    "    print (logbook, flush=True)\n",
    "    print (hof, flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
